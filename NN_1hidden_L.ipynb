{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\"> Convex optimization for machine learning: Part 2</div>\n",
    "#### <div style=\"text-align: right\"> Prof. Changho Suh, TA Gyeongjo Hwang, Doyeon Kim</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement XOR function using linear classifier\n",
    "\n",
    " **XOR table:**\n",
    " \n",
    " <img src=\"figures/XOR_table.png\" style=\"width:270px;height:150px;\">\n",
    " \n",
    " **XOR problem:** \n",
    " \n",
    " 앞서 구현한 One layer neural network로 XOR gate 구현이 불가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from OL_utils import sigmoid, Logistic, plot\n",
    "plt.rcParams['figure.figsize'] = (11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XOR inputs & Corresponding outputs\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y_train = np.array([0, 1, 1, 0])\n",
    "# Visualizing XOR gate\n",
    "xlim = (-.5, 1.5)\n",
    "ylim = (-.5, 1.5)\n",
    "plot(X_train, Y_train, title='XOR gate', s=100, axis=True, xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.693147\n",
      "Cost after iteration 200: 0.693147\n",
      "Cost after iteration 300: 0.693147\n",
      "Cost after iteration 400: 0.693147\n",
      "Cost after iteration 500: 0.693147\n",
      "Cost after iteration 600: 0.693147\n",
      "Cost after iteration 700: 0.693147\n",
      "Cost after iteration 800: 0.693147\n",
      "Cost after iteration 900: 0.693147\n",
      "Cost after iteration 1000: 0.693147\n",
      "Cost after iteration 1100: 0.693147\n",
      "Cost after iteration 1200: 0.693147\n",
      "Cost after iteration 1300: 0.693147\n",
      "Cost after iteration 1400: 0.693147\n",
      "Cost after iteration 1500: 0.693147\n",
      "Cost after iteration 1600: 0.693147\n",
      "Cost after iteration 1700: 0.693147\n",
      "Cost after iteration 1800: 0.693147\n",
      "Cost after iteration 1900: 0.693147\n",
      "train accuracy: 50.0 %\n",
      "test accuracy: 50.0 %\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "d = Logistic(X_train, Y_train, X_train, Y_train, num_iterations=2000, learning_rate=0.1, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_boundary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f7496f9db1e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdraw_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'XOR gate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'draw_boundary' is not defined"
     ]
    }
   ],
   "source": [
    "w = d['w']\n",
    "b = d['b']\n",
    "draw_boundary(w, b, X_train, Y_train, title='XOR gate', colormap=False, s=100, axis=True, xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 시간에 구현한 Least Square classifier, Logistic Regression은 모두 선형분류기(Linear classifier)이므로 위와 같이 Non-linear한 데이터에 대해서는 학습이 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 신경망 모델(Neural Network model)\n",
    "\n",
    "로지스틱 회귀(Logistic Regression)은 기본적으로 선형 판별자(Linear Classifier)이기 때문에 비선형 데이터(Non-linear dataset)에 대해서는 잘 동작하지 않는다. 이를 해결하기 위해 hidden layer 1층을 추가한 신경망(Nueral Network)을 구현할 것이다. \n",
    "\n",
    "**Here is our model**:\n",
    "<img src=\"figures/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "각각의 데이터 $x^{(i)}$에 대해:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = ReLU(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "데이터셋에 대해 손실함수(Loss function):\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Reminder**: 신경망을 구현하기 위한 Steps:\n",
    "    1. 신경망 구조를 정의 (e.g., number of input units,  number of hidden units, etc). \n",
    "    2. 모델 파라미터 초기화\n",
    "    3. Loop:\n",
    "        - forward 전파\n",
    "        - 손실 값 계산\n",
    "        - 그레디언트 계산을 위한 backward 전파\n",
    "        - 파라미터 업데이트(Gradient descent algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**시그모이드의 한계점**\n",
    "\n",
    "<img src=\"figures/sigmoid.png\" style=\"width:300px;height:300px;\">\n",
    "\n",
    "\n",
    "위 그래프에서 알 수 있듯이, x의 값이 양 끝으로 향하면 그 지역의 기울기는 매우 작아서 거의 0에 가까워진다. 즉, y의 값들이 x의 반응에 덜 반응하는 경향이 있는데, 이는 'Gradient Vanishing'이라는 문제를 야기시킨다. Backward 연산동안, 이러한 0에 가까운 기울기가 계속 곱해짐에 따라 그레디언트가 거의 0에 수렴하여 네트워크는 더이상 학습이 어려워지거나 급격하게 천천히 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why ReLU?\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/relu.png\" style=\"width:300px;height:300px;\">\n",
    "\n",
    "ReLU는 그래프에서 보이는 것처럼 0보다 작을 때 계속 0의 값을 가지다가, 0의 값 이상일 때 선형 관계를 가지는 형태이다. 이는 0보다 큰 값에 대해서는 그래디언트가 계속 1의 값을 가지기 때문에 앞서 언급한 'Gradient Vanishing'문제는 어느정도 해결할 수 있다.\n",
    "\n",
    "하지만 음수의 활성은 업데이트 하지 못하는 문제가 있고, 목적함수가 볼록함수(Convex)가 아니므로 'Local optimal' 지점이 존재하여 이는 학습을 더 어렵게 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 함수정의\n",
    "def ReLU(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - 신경망의 구조 정의 ####\n",
    "\n",
    "**실습:** 다음 세개의 변수를 지정하시오:\n",
    "    - n_x: 입력단(input layer)의 크기(number of units)\n",
    "    - n_h: hidden layer의 크기(number of units) --> (4로 세팅) \n",
    "    - n_y: 출력단(output layer)의 크기(number of units)\n",
    "\n",
    "**힌트:** n_x와 n_y는 X와 Y의 shape을 이용해서 구하고 n_h는 직접 손으로 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"   \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess = layer_sizes_test_case()\n",
    "(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - 모델 파라미터 초기화(Initialization) ####\n",
    "\n",
    "**실습:** `initialize_parameters()`함수 구현\n",
    "\n",
    "**Instructions**:\n",
    "- 파라미터 shape 체크하기\n",
    "- 파라미터의 각 요소는 랜덤 값으로 initialization\n",
    "    - Use: `np.random.randn(a,b) * 0.01` ,(a,b)는 각 파라미터 shape.\n",
    "- bias는 0으로 initialization\n",
    "    - Use: `np.zeros((a,b))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2019)\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "#     W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "#     b1 = np.zeros((n_h, 1))\n",
    "#     W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "#     b2 = np.zeros((n_y, 1))\n",
    "    W1 = np.random.randn(n_h, n_x)\n",
    "    b1 = np.random.randn(n_h, 1)\n",
    "    W2 = np.random.randn(1, n_h)\n",
    "    b2 = np.random.randn(1, 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # 파라미터 shape 체크\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = initialize_parameters_test_case()\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td>W1 :</td>\n",
    "    <td> [[-0.00416758 -0.00056267]\n",
    " [-0.02136196  0.01640271]\n",
    " [-0.01793436 -0.00841747]\n",
    " [ 0.00502881 -0.01245288]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>b1 :</td>\n",
    "    <td> [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>W2 :</td>\n",
    "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>b2 :</td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - The Loop ####\n",
    "\n",
    "**실습:** `forward()`함수를 구현하시오\n",
    "\n",
    "**Instructions**:\n",
    "- 위의 mathematical 수식 부분 참조\n",
    "- `sigmoid()` 함수는 OL_utils.py에서 일괄적으로 임포트\n",
    "- `ReLU()`함수 사용\n",
    "- 단계적으로 구현해야 할 것:\n",
    "    1. 로지스틱 구현할때와 마찬가지로 `parameters[\"..\"]`딕셔너리로부터 파라미터를 추출 (`initialize_parameters()`함수의 출력)\n",
    "    2. forward를 구현하고 $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}(=\\hat{Y})$를 계산 (학습데이터셋에 대한 예측(prediction)의 벡터를 나타낸다).\n",
    "- backward 전파 연산에 필요한 값들은 \"`cache`\"에 저장. 이 `cache`는 backward()함수의 인풋으로 들어갈 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2(=Yhat) (probabilities)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, parameters = forward_propagation_test_case()\n",
    "\n",
    "A2, cache = forward(X_assess, parameters)\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "print(np.mean(cache['Z1']), np.mean(cache['A1']), np.mean(cache['Z2']), np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table style=\"width:55%\">\n",
    "  <tr>\n",
    "    <td> -0.000499755777742 -0.000496963353232 0.000438187450959 0.500109546852 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "위에서 계산한 $A^{[2]}(=\\hat{Y})$ (in the Python variable \"`A2`\")는 모든 데이터에 대한 확률예측값인 $a^{[2](i)}(=\\hat{y}^{(i)})$의 벡터이다. 손실함수는 다음과 같이 계산한다.\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(\\hat{y}^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat{y}^{(i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    "**실습:**\n",
    "\n",
    "$J$ 계산을 위한`compute_cost()`함수를 구현하시오\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "$- \\sum\\limits_{i=1}^{m}  y^{(i)}\\log(\\hat{y}^{(i)})$ 식을 구현하기 위한 코드는 아래와 같다.\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = - np.sum(logprobs)                # 루프없이 행렬연산으로만 구현가능하다\n",
    "```\n",
    "\n",
    "(바로 `np.dot()`함수를 써도 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "    \n",
    "    # Retrieve W1 and W2 from parameters\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Compute the cross-entropy cost\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)     # 파이썬 숫자로 출력하기 위함\n",
    "                                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2, Y_assess, parameters = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td>cost :</td>\n",
    "    <td> 0.692919893776 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward동안 계산된 cache를 이용하여 backward propagation 연산을 구현할 수 있다.\n",
    "\n",
    "**실습 :** `backward()` 함수를 구현하시오.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "실제로 backpropagation연산 구현은 딥러닝에서 굉장히 어려운 일에 속한다. 하단 오른편 6개의 식을 참고하여 구현한다. (왼편 식의 vectorized version)\n",
    "\n",
    "<img src=\"figures/grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (\\hat{y}^{(i)} - y^{(i)})$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * \\mathbb 1(z_1^{(i)} \\geq 0) $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n",
    "\n",
    "- $*$ 는 요소별 곱을 의미\n",
    "- 각 그래디언트를 나타내는 notation들은 다음과 같다\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1, Z1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A1 = cache['A1']\n",
    "    Z1 = cache['Z1']\n",
    "    A2 = cache['A2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### (≈ 8 lines of code, corresponding to 6 equations on slide above)\n",
    "    dZ2= (A2 - Y) / m\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dA1[Z1 < 0] = 0\n",
    "    dZ1 = dA1\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
    "\n",
    "grads = backward(parameters, cache, X_assess, Y_assess)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>dW1 :</td>\n",
    "    <td> [[ 0.01018708 -0.00708701]\n",
    " [ 0.00873447 -0.0060768 ]\n",
    " [-0.00530847  0.00369379]\n",
    " [-0.02206365  0.01535126]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>db1 :</td>\n",
    "    <td>  [[-0.00069728]\n",
    " [-0.00060606]\n",
    " [ 0.000364  ]\n",
    " [ 0.00151207]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>dW2 :</td>\n",
    "    <td> [[ 0.00363613  0.03153604  0.01162914 -0.01318316]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>db2 :</td>\n",
    "    <td> [[ 0.06589489]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습 :** 경사하강 알고리즘(SGD)을 이용하여 업데이트 함수를 정의하시오. (위에서 계산한(dW1, db1, dW2, db2)를 이용하여 파라미터 (W1, b1, W2, b2)를 업데이트)\n",
    "\n",
    "**General gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$  ($\\alpha$는 learning rate,  $\\theta$는 각 파라미터를 의미한다.\n",
    "\n",
    "**Note**: 초기 learning rate값을 어떻게 잡느냐에 따라 학습 후 성능에 큰 영향을 미칠 수 있다.\n",
    "\n",
    "<img src=\"figures/sgd.gif\" style=\"width:400;height:400;\"> <img src=\"figures/sgd_bad.gif\" style=\"width:400;height:400;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=0.01):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, learning_rate=1.2)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>W1 :</td>\n",
    "    <td> [[-0.00643025  0.01936718]\n",
    " [-0.02410458  0.03978052]\n",
    " [-0.01653973 -0.02096177]\n",
    " [ 0.01046864 -0.05990141]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>b1 :</td>\n",
    "    <td> [[ -1.02420756e-06]\n",
    " [  1.27373948e-05]\n",
    " [  8.32996807e-07]\n",
    " [ -3.20136836e-06]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>W2 :</td>\n",
    "    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>b2 :</td>\n",
    "    <td> [[ 0.00010457]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - 4.1, 4.2, 4.3에서 구현한 코드 NN_2L()함수로 합치기 ####\n",
    "\n",
    "위에서 neural network를 학습하기 위한 함수들을 하나의`NN_2L()`함수로 구현\n",
    "- X, Y, n_h, num_iterations 등을 인수로 받고 파라미터 딕셔너리를 리턴\n",
    "- Loop statement를 이용해 num_iterations만큼 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_2L(X, Y, n_h, num_iterations=10000, learning_rate=0.01, print_cost=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    print(n_x, n_y)\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    print(W1)\n",
    "    print(b1)\n",
    "    print(W2)\n",
    "    print(b2)\n",
    "\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess = nn_model_test_case()\n",
    "\n",
    "parameters = NN_2L(X_assess, Y_assess, 4, num_iterations=10000, learning_rate=1.2, print_cost=False)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td>W1 :</td>\n",
    "    <td> [[-4.18494056  5.33220609]\n",
    " [-7.52989382  1.24306181]\n",
    " [-4.1929459   5.32632331]\n",
    " [ 7.52983719 -1.24309422]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>b1 :</td>\n",
    "    <td> [[ 2.32926819]\n",
    " [ 3.79458998]\n",
    " [ 2.33002577]\n",
    " [-3.79468846]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>W2 :</td>\n",
    "    <td> [[-6033.83672146 -6008.12980822 -6033.10095287  6008.06637269]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>b2 :</td>\n",
    "    <td> [[-52.66607724]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 데이터 클래스 예측(prediction)\n",
    "\n",
    "**실습 :** 클래스를 예측하는 predict()함수를 구현하시오.\n",
    "\n",
    " - forward함수에서 출력된 값을 이용한다.\n",
    "\n",
    "**Reminder**: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    \n",
    "$\\hat{Y}$값을 thredhold값에 따라 prediction에 1또는 0을 넣어주고 싶다면\n",
    "\n",
    "```X_new = (X > threshold)```\n",
    "\n",
    "를 이용하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, X_assess = predict_test_case()\n",
    "\n",
    "predictions = predict(parameters, X_assess)\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>predictions mean :</td>\n",
    "    <td> 0.666666666667 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Synthetic Data 학습\n",
    "\n",
    " - XOR function\n",
    " - Circle Dataset\n",
    " - Moon shaped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR inputs & Corresponding outputs\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y_train = np.array([[0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = NN_2L(X_train, Y_train, n_h=2, num_iterations=10000, learning_rate=0.1, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boundary_nn(parameters, X_train, Y_train,  title='XOR gate', s=100, axis=True, xlim=xlim, ylim=ylim, colormap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
